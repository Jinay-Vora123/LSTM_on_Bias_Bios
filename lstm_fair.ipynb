{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6002003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc16ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"bias_bio.csv\", index_col=0)\n",
    "texts = df['hard_text'].astype(str).tolist()\n",
    "y = df['gender'].values\n",
    "S = df['profession'].values\n",
    "\n",
    "tok   = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert  = AutoModel.from_pretrained('bert-base-uncased').eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert.to(device)\n",
    "\n",
    "def embed_texts(texts, batch_size=32, max_len=128):\n",
    "    all_emb = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc   = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            out  = bert(**enc).last_hidden_state\n",
    "            mask = enc['attention_mask'].unsqueeze(-1)\n",
    "            summed = (out * mask).sum(dim=1)\n",
    "            counts = mask.sum(dim=1)\n",
    "            emb    = (summed / counts).cpu()\n",
    "        all_emb.append(emb)\n",
    "    return torch.cat(all_emb, dim=0)\n",
    "\n",
    "X = embed_texts(texts)\n",
    "\n",
    "# 3) Define the LSTM class (must match your training code)\n",
    "class OneStepLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc   = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        out,(hn,_) = self.lstm(x)\n",
    "        return self.fc(hn[-1])\n",
    "\n",
    "# 4) Load pretrained “unfair” model\n",
    "model_unfair = OneStepLSTM(X.size(1), 128, 1, 2).to(device)\n",
    "chk = torch.load(\"lstm_gender_classifier.pth\", map_location=device)\n",
    "model_unfair.load_state_dict(chk['model_state_dict'])\n",
    "model_unfair.eval()\n",
    "\n",
    "# 5) Load pretrained “fair” model\n",
    "model_fair = OneStepLSTM(X.size(1), 128, 1, 2).to(device)\n",
    "chk = torch.load(\"fair_lstm_gender_classifier.pth\", map_location=device)\n",
    "model_fair.load_state_dict(chk['model_state_dict'])\n",
    "model_fair.eval()\n",
    "\n",
    "# 6) Batch‐predict helper\n",
    "def get_preds(model, X, batch_size=64):\n",
    "    preds = []\n",
    "    loader = DataLoader(TensorDataset(X), batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for xb, in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "    return np.array(preds)\n",
    "\n",
    "pred_unfair = get_preds(model_unfair, X)\n",
    "pred_fair   = get_preds(model_fair,   X)\n",
    "\n",
    "# 7) Build AIF360 datasets\n",
    "df_bld = pd.DataFrame({'gender': y, 'profession': S})\n",
    "bld = BinaryLabelDataset(\n",
    "    df=df_bld,\n",
    "    label_names=['gender'],\n",
    "    protected_attribute_names=['profession'],\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0\n",
    ")\n",
    "\n",
    "pred_unfair_bld = bld.copy()\n",
    "pred_unfair_bld.labels = pred_unfair.reshape(-1,1)\n",
    "pred_fair_bld   = bld.copy()\n",
    "pred_fair_bld.labels   = pred_fair.reshape(-1,1)\n",
    "\n",
    "# 8) Define privileged vs. unprivileged\n",
    "priv_code = int(df_bld['profession'].value_counts().idxmax())\n",
    "other     = [int(c) for c in df_bld['profession'].unique() if c!=priv_code]\n",
    "privileged_groups   = [{'profession': priv_code}]\n",
    "unprivileged_groups = [{'profession': c} for c in other]\n",
    "\n",
    "# 9) Compute metrics\n",
    "m_unfair = ClassificationMetric(\n",
    "    bld, pred_unfair_bld,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "m_fair = ClassificationMetric(\n",
    "    bld, pred_fair_bld,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "acc_u  = m_unfair.accuracy()\n",
    "spd_u  = m_unfair.statistical_parity_difference()\n",
    "aod_u  = m_unfair.average_odds_difference()\n",
    "\n",
    "acc_f  = m_fair.accuracy()\n",
    "spd_f  = m_fair.statistical_parity_difference()\n",
    "aod_f  = m_fair.average_odds_difference()\n",
    "\n",
    "# 10) Plot comparison\n",
    "metrics = ['Accuracy','Statistical Parity Diff','Average Odds Diff']\n",
    "vals_unfair = [acc_u, spd_u, aod_u]\n",
    "vals_fair   = [acc_f, spd_f, aod_f]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x - width/2, vals_unfair, width, label='Unfair')\n",
    "ax.bar(x + width/2, vals_fair,   width, label='Fair')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
